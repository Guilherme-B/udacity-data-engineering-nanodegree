# Table of Contents
1. [Introduction](#introduction)
2. [Dataset](#dataset)
3. [DAG](#dag)
4. [Project Structure](#projectstructure)


## Introduction <a name="introduction"></a>

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They have decided to bring you into the project and expect you to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.

The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

The project revolves around Apache Airflow and AWS's S3 and Redshift:

 1. AWS S3 - Amazon S3 or Amazon Simple Storage Service is a service
    offered by Amazon Web Services (AWS) that provides object storage through a web service interface
 
 2. Apache Airflow - Apache Airflow is an open-source workflow management platform. It started at Airbnb in October 2014 as a solution to manage the company's increasingly complex workflows. Creating Airflow allowed Airbnb to programmatically author and schedule their workflows and monitor them via the built-in Airflow user interface.From the beginning, the project was made open source, becoming an Apache Incubator project in March 2016 and a Top-Level Apache Software Foundation project in January 2019. Airflow is written in Python, and workflows are created via Python scripts. Airflow is designed under the principle of "configuration as code". While other "configuration as code" workflow platforms exist using markup languages like XML, using Python allows developers to import libraries and classes to help them create their workflows.
 3. AWS Redshift - Amazon Redshift is a data warehouse product which forms part of the larger cloud-computing platform Amazon Web Services. It is built on top of technology from the massive parallel processing (MPP) data warehouse company ParAccel to handle large scale data sets and database migrations. Redshift differs from Amazon's other hosted database offering, Amazon RDS, in its ability to handle analytic workloads on big data data sets stored by a column-oriented DBMS principle. Redshift allows up to 16 petabytes of data on a cluster compared to Amazon RDS's maximum database size of 16TB.

## Dataset

The data is located in two S3 buckets:

1. song data: `s3://udacity-dend/song_data`
2. log data: `s3://udacity-dend/log_data`

### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, `TRAABJL12903CDCF1A.json`, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset
The second dataset consists of log files in JSON format generated by an event simulator  based on the songs in songs dataset. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

Below is an example of what the data in a log file, `2018-11-12-events.json`, looks like:
![log-dataset-example](https://video.udacity-data.com/topher/2019/February/5c6c3ce5_log-data/log-data.png)

## (Parquet) DAG <a name="dag"></a>

The project revolves around the creation of a specific DAG architecture provided by the client.

![Data Model](https://raw.githubusercontent.com/Guilherme-B/udacity-data-engineering-nanodegree/master/4.airflow/images/dag.png)
![Data Model](https://raw.githubusercontent.com/Guilherme-B/udacity-data-engineering-nanodegree/master/4.airflow/images/tasks.png)

### Operators <a name="operators"></a>

The project required the creation of three Airflow operators:
- Stage Operator - The stage operator is expected to be able to load any JSON formatted files from S3 to Amazon Redshift. The operator creates and runs a SQL COPY statement based on the parameters provided. The operator's parameters should specify where in S3 the file is loaded and what is the target table.
- Fact and Dimension Operators - With dimension and fact operators, you can utilize the provided SQL helper class to run data transformations. Most of the logic is within the SQL transformations and the operator is expected to take as input a SQL statement and target database on which to run the query against. You can also define a target table that will contain the results of the transformation. Dimension loads are often done with the truncate-insert pattern where the target table is emptied before the load. Thus, you could also have a parameter that allows switching between insert modes when loading dimensions. Fact tables are usually so massive that they should only allow append type functionality.
- Data Quality Operator - The data quality operator is used to run checks on the data itself. The operator's main functionality is to receive one or more SQL based test cases along with the expected results and execute the tests. For each the test, the test result and expected result needs to be checked and if there is no match, the operator should raise an exception and the task should retry and fail eventually. For example one test could be a SQL statement that checks if certain column contains NULL values by counting all the rows that have NULL in the column. We do not want to have any NULLs so expected result would be 0 and the test would compare the SQL statement's outcome to the expected result.


## Data Model <a name="datamodel"></a>

The required data model is a star-schema comprised by one fact table, songplays, which records the songs played by users, along with four dimensions, users, songs, artists and finally, time.

![Data Model](https://raw.githubusercontent.com/Guilherme-B/udacity-data-engineering-nanodegree/master/2.data%20modeling/data%20modeling%20with%20postgres/images/star_schema.png)


  
| Table| Type| Dependencies | Description |
|---|---|---| --- | 
|   songplays| Fact| song, artist, user, time| Records in log data associated with song plays |
|   song|   Dimension| artist  | Represents an artist's song |
|   artist|   Dimension| none  | Represents a given artist |
|   user|   Dimension | none | Represents a platform user |
|   time|   Dimension | none | The time dimension |
 

### Fact Table
#####  songplays
 
| Column| Type| Constraints|
|---|---|---|
|   songplay_id| int IDENTITY(1,0) |  Primary Key| 
|   start_time|   bigint| Foreign Key time(start_time), **SortKey**| 
|   user_id|   int|   Foreign Key users(user_id), **DistKey**| 
|   level|   varchar || 
|   song_id|   varchar| Foreign Key songs(song_id)| 
|   artist_id|   varchar|Foreign Key artists(artist_id)| 
|   session_id|   int|| 
|   location|   text|| 
|   user_agent|   text|| 
 

### Dimension Tables

#### user

| Column| Type| Constraints|
|---|---|---|
|   user_id| serial | Primary Key, **DistKey**| 
|   first_name|   varchar|| 
|   last_name|   varchar|| 
|   gender|   varchar(1) || 
|   level|   varchar|| 
 


#### songs

 | Column| Type| Constraints|
|---|---|---|
|   song_id| varchar|  Primary Key, **SortKey** | 
|   title|   varchar|| 
|   artist_id|   varchar|| 
|   year|   int || 
|   duration	|   decimal	|| 
  

                
#### artists
 | Column| Type| Constraints|
|---|---|---|
|   artist_id| varchar| Primary Key, **SortKey**| 
|   name|   varchar|NOT NULL| 
|   location|   varchar|| 
|   latitude|   decimal|| 
|   longitude|   decimal || 

 

#### time

| Column| Type| Constraints|
|---|---|---|
|   start_time| timestamp| Primary Key, **Sortkey**| 
|   hour|   int| NOT NULL | 
|   day|   int|NOT NULL| 
|   week|   int|NOT NULL| 
|   month|   int|NOT NULL| 
|   year|   int|NOT NULL| 
|   weekday|   varchar|NOT NULL|

### Execution <a name="execution"></a>

The entire process runs in under one minute by offloading the actual processing and heavy workload onto the Redshift cluster, with Airflow being in charge solely of orchestration and task lifecycle management and communication.

![DAG Gantt](https://raw.githubusercontent.com/Guilherme-B/udacity-data-engineering-nanodegree/master/2.data%20modeling/data%20modeling%20with%20postgres/images/gantt.png)
