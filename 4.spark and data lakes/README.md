# Table of Contents
1. [Introduction](#introduction)
2. [Dataset](#dataset)
3. [Data Model](#datamodel)
4. [Project Structure](#projectstructure)


## Introduction <a name="introduction"></a>

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

The project revolves around two AWS services and Apache Spark:

 1. AWS S3 - Amazon S3 or Amazon Simple Storage Service is a service
    offered by Amazon Web Services (AWS) that provides object storage through a web service interface
 
 2. AWS EMR - hosting Spark - Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. Amazon EMR makes it easy to set up, operate, and scale big data environments by automating time-consuming tasks like provisioning capacity and tuning clusters. EMR allows running petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. Workloads can be executed on Amazon EC2 instances, on Amazon Elastic Kubernetes Service (EKS) clusters, or on-premises using EMR on AWS Outposts.
 
 3. Apache Spark - Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.
 
 ## Dataset

The data is located in two S3 buckets:

1. song data: `s3://udacity-dend/song_data`
2. log data: `s3://udacity-dend/log_data`

### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, `TRAABJL12903CDCF1A.json`, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset
The second dataset consists of log files in JSON format generated by an event simulator  based on the songs in songs dataset. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

Below is an example of what the data in a log file, `2018-11-12-events.json`, looks like:
![log-dataset-example](https://video.udacity-data.com/topher/2019/February/5c6c3ce5_log-data/log-data.png)



## (Parquet) Data Model <a name="datamodel"></a>

The required data model is a star-schema comprised by one fact table, songplays, which records the songs played by users, along with four dimensions, users, songs, artists and finally, time. Notice that the project does not rely on external DB engines, using Parquet files instead, which are directly deployed in an S3 bucket.

![Data Model](https://raw.githubusercontent.com/Guilherme-B/udacity-data-engineering-nanodegree/master/2.data%20modeling/data%20modeling%20with%20postgres/images/star_schema.png)


  
| Table| Type| Dependencies | Description |
|---|---|---| --- | 
|   songplays| Fact| song, artist, user, time| Records in log data associated with song plays |
|   song|   Dimension| artist  | Represents an artist's song |
|   artist|   Dimension| none  | Represents a given artist |
|   user|   Dimension | none | Represents a platform user |
|   time|   Dimension | none | The time dimension |
 

### Fact Table
#####  songplays
  
```
root
 |-- start_time: timestamp (nullable = true)
 |-- userId: string (nullable = true)
 |-- level: string (nullable = true)
 |-- sessionId: long (nullable = true)
 |-- location: string (nullable = true)
 |-- userAgent: string (nullable = true)
 |-- song_id: string (nullable = true)
 |-- artist_id: string (nullable = true)
 |-- songplay_id: long (nullable = false)
 
Partition By (year, month)
```

### Dimension Tables

#### users

```
root
 |-- firstName: string (nullable = true)
 |-- lastName: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- level: string (nullable = true)
 |-- userId: string (nullable = true)
```


#### songs
  
```
root
 |-- song_id: string (nullable = true)
 |-- title: string (nullable = true)
 |-- artist_id: string (nullable = true)
 |-- year: long (nullable = true)
 |-- duration: double (nullable = true)
 
Partition By (year, artist_id)
```

                
#### artists
```
root
 |-- artist_id: string (nullable = true)
 |-- artist_name: string (nullable = true)
 |-- artist_location: string (nullable = true)
 |-- artist_latitude: double (nullable = true)
 |-- artist_longitude: double (nullable = true)
``` 

#### time
```
root
 |-- start_time: timestamp (nullable = true)
 |-- hour: integer (nullable = true)
 |-- day: integer (nullable = true)
 |-- week: integer (nullable = true)
 |-- month: integer (nullable = true)
 |-- year: integer (nullable = true)
 |-- weekday: integer (nullable = true)
```

### Project Structure <a name="projectstructure"></a>

The project is comprised of two key elements, and a helper:
- **etl.py** - Processes the log and song JSON files and dumps the corresponding Parquet data model
- **dl.cfg** - Contains AWS auth configuration
- **EMR Manager.ipynb** - Assists in launching and deploying the **etl.py** into EMR using AWS's CLI.

### The ETL process (etl.py)

The ETL process is divided into two main steps:

1. Process the song JSON objects
    1. Create the **songs** dimension table and dump the corresponding Parquet file to an S3 bucket
    2. Create the **artists** dimension table and dump the corresponding Parquet file to an S3 bucket
2. Process the log JSON objects
    1. Create the **songplay** fact table and dump the corresponding Parquet file to an S3 bucket
    2. Create the **user** dimension table and dump the corresponding Parquet file to an S3 bucket
    3. Create the **time** dimension table and dump the corresponding Parquet file to an S3 bucket