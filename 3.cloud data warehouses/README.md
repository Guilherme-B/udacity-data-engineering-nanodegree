
# Table of Contents
1. [Introduction](#introduction)
2. [Dataset](#dataset)
3. [Data Model](#datamodel)
4. [Project Structure](#projectstructure)


## Introduction <a name="introduction"></a>

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to. You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

The project revolves around two AWS services:

 1. AWS S3 - Amazon S3 or Amazon Simple Storage Service is a service
    offered by Amazon Web Services (AWS) that provides object storage through a web service interface
 
 2. AWS Redshift - AWS Redshift - Amazon Redshift is a data warehouse product which forms part of the larger cloud-computing platform Amazon Web Services. It is built on top of  technology from the massive parallel processing (MPP) data warehouse company ParAccel to handle large scale data sets and database migrations. Redshift differs from Amazon's other hosted database offering, Amazon RDS, in its ability to handle analytic workloads on big data data sets stored by a column-oriented DBMS principle. Redshift allows up to 16 petabytes of data on a cluster compared to Amazon RDS's maximum database size of 16TB.

## Dataset <a name="dataset"></a>

The data is located in three S3 buckets:

 1. song data: `s3://udacity-dend/song_data`
 2. log data: `s3://udacity-dend/log_data`
 3. log JSON `map: s3://udacity-dend/log_json_path.json`

### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, `TRAABJL12903CDCF1A.json`, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset
The second dataset consists of log files in JSON format generated by an event simulator  based on the songs in songs dataset. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

Below is an example of what the data in a log file, `2018-11-12-events.json`, looks like:
![enter image description here](https://video.udacity-data.com/topher/2019/February/5c6c3ce5_log-data/log-data.png)

## Data Model <a name="datamodel"></a>

The required data model is a star-schema comprised by one fact table, songplays, which records the songs played by users, along with four dimensions, users, songs, artists and finally, time.

![Data Model](https://raw.githubusercontent.com/Guilherme-B/udacity-data-engineering-nanodegree/master/2.data%20modeling/data%20modeling%20with%20postgres/images/star_schema.png)


  
| Table| Type| Dependencies | Description |
|---|---|---| --- | 
|   songplays| Fact| song, artist, user, time| Records in log data associated with song plays |
|   song|   Dimension| artist  | Represents an artist's song |
|   artist|   Dimension| none  | Represents a given artist |
|   user|   Dimension | none | Represents a platform user |
|   time|   Dimension | none | The time dimension |
 

### Fact Table
#####  songplays
 
| Column| Type| Constraints|
|---|---|---|
|   songplay_id| int IDENTITY(1,0) |  Primary Key| 
|   start_time|   bigint| Foreign Key time(start_time), **SortKey**| 
|   user_id|   int|   Foreign Key users(user_id), **DistKey**| 
|   level|   varchar || 
|   song_id|   varchar| Foreign Key songs(song_id)| 
|   artist_id|   varchar|Foreign Key artists(artist_id)| 
|   session_id|   int|| 
|   location|   text|| 
|   user_agent|   text|| 
 

### Dimension Tables

#### user

| Column| Type| Constraints|
|---|---|---|
|   user_id| serial | Primary Key, **DistKey**| 
|   first_name|   varchar|| 
|   last_name|   varchar|| 
|   gender|   varchar(1) || 
|   level|   varchar|| 
 


#### songs

 | Column| Type| Constraints|
|---|---|---|
|   song_id| varchar|  Primary Key, **SortKey** | 
|   title|   varchar|| 
|   artist_id|   varchar|| 
|   year|   int || 
|   duration	|   decimal	|| 
  

                
#### artists
 | Column| Type| Constraints|
|---|---|---|
|   artist_id| varchar| Primary Key, **SortKey**| 
|   name|   varchar|NOT NULL| 
|   location|   varchar|| 
|   latitude|   decimal|| 
|   longitude|   decimal || 

 

#### time

| Column| Type| Constraints|
|---|---|---|
|   start_time| timestamp| Primary Key, **Sortkey**| 
|   hour|   int| NOT NULL | 
|   day|   int|NOT NULL| 
|   week|   int|NOT NULL| 
|   month|   int|NOT NULL| 
|   year|   int|NOT NULL| 
|   weekday|   varchar|NOT NULL|

### Project Structure <a name="projectstructure"></a>

The project is comprised of three key files:
- **sql_queries.py** - Contains SQL query definition, from table creation to data insertion and analysis
- **create_tables.py** - Executes the queries required to setup the database and the model's tables
- **etl.py** - Processes the log and song files and inserts data into the Redshift

### The ETL process (etl.py)

The ETL process is divided into four main steps:

1. Process the  JSON objects into a **staging** schema
    1. COPY the **log_data** to Redshift
    2. COPY the **song_data** to Redshift
2. Use the **staging** data to populate the **dimensional model**
    1. Populate the **songplay** fact table
    2. Populate the **user** dimension
    3. Populate the **song** dimension
    4. Populate the **artist** dimension
    5. Populate the **time** dimension
4. Close the database connection